{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Download Datasets\n",
    "\n",
    "We get the datasets from open access papers on IEEE Explore."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "#!apt install chromium-chromedriver\n",
    "##!cp /usr/lib/chromium-browser/chromedriver /usr/local/bin\n",
    "#!sudo cp /usr/lib/chromium-browser/chromedriver /usr/local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import time\n",
    "from random import shuffle\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "\n",
    "cnn_paper_urls = 'cnn_papers.txt'\n",
    "nlp_paper_urls = 'nlp_papers.txt'\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#cnn_paper_urls = '/content/gdrive/My Drive/NLP/Project/cnn_papers.txt'\n",
    "#nlp_paper_urls = '/content/gdrive/My Drive/NLP/Project/nlp_papers.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_body(body):\n",
    "    \"\"\"This function takes the body of the paper and returns three lists: the section numbers (in roman numerals), the section titles, and the section text\n",
    "    It was written entirely by GitHub Copilot\"\"\"\n",
    "    sections = body.split('SECTION ')\n",
    "\n",
    "    sections = sections[1:] # remove the first element, which is just an empty string due to how we split the text\n",
    "    section_nums = []\n",
    "    section_titles = []\n",
    "    section_texts = []\n",
    "    for section in sections:\n",
    "        section_num = section.split('\\n')[0]\n",
    "        section_title = section.split('\\n')[1]\n",
    "        section_text = section.split('\\n')[2:]\n",
    "\n",
    "        section_text = ' '.join(section_text)\n",
    "        section_nums.append(section_num)\n",
    "        section_titles.append(section_title)\n",
    "        section_texts.append(section_text)\n",
    "\n",
    "    return section_nums, section_titles, section_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_paper(url):\n",
    "    driver.get(url)\n",
    "\n",
    "    #title = driver.find_element_by_class_name('document-title').text\n",
    "    title = driver.find_element(By.CLASS_NAME, 'document-title').text\n",
    "\n",
    "    # the 2nd u-mb-1 div is normally the abstract, but sometimes it is empty, so we have to handle that special case\n",
    "    #abstract = driver.find_elements_by_class_name('u-mb-1')[1]\n",
    "    abstract = driver.find_element(By.CLASS_NAME, 'u-mb-1')\n",
    "    \n",
    "    if abstract.text == '':\n",
    "        #abstract = driver.find_elements_by_class_name('u-mb-1')[2]\n",
    "        abstract = driver.find_element(By.CLASS_NAME, 'u-mb-1')\n",
    "        \n",
    "    #abstract = abstract.find_elements_by_tag_name('div')[0].text\n",
    "    abstract = abstract.find_elements(By.TAG_NAME, 'div')[0].text\n",
    "\n",
    "    try:\n",
    "        body = driver.find_element('id', 'article').text\n",
    "    except:\n",
    "        time.sleep(2) # wait a second and try again, the page probably just needs more time to load\n",
    "        body = driver.find_element('id', 'article').text\n",
    "\n",
    "    # sometimes, the html of the page loads, but it takes even longer to load the text, so wait just a tiny bit more\n",
    "    if body == '':\n",
    "        time.sleep(3)\n",
    "        body = driver.find_element('id', 'article').text\n",
    "        if body == '':\n",
    "            raise Exception('Could not load body of paper')\n",
    "\n",
    "    _, section_titles, section_texts = parse_body(body)\n",
    "\n",
    "    return title, abstract, section_titles, section_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_papers(url_file):\n",
    "    with open(url_file, 'r') as f:\n",
    "        urls = f.readlines()\n",
    "    urls = [url.strip() for url in urls]\n",
    "\n",
    "    papers = []\n",
    "    for url in tqdm(urls, desc=f'Scraping papers from {url_file}', total=len(urls), unit='paper'):\n",
    "        try:\n",
    "            title, abstract, section_titles, section_texts = scrape_paper(url)\n",
    "            papers.append({'title': title, 'abstract': abstract, 'section_titles': section_titles, 'section_texts': section_texts})\n",
    "        except Exception as e:\n",
    "            print(f'Failed to scrape {url}, error: {e}')\n",
    "            continue\n",
    "\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping papers from cnn_papers.txt: 100%|███| 50/50 [05:25<00:00,  6.52s/paper]\n"
     ]
    }
   ],
   "source": [
    "cnn_papers = scrape_papers(cnn_paper_urls)\n",
    "\n",
    "# save the papers to a json file\n",
    "with open('cnn_papers.json', 'w') as f:\n",
    "    json.dump(cnn_papers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping papers from nlp_papers.txt:  60%|█▊ | 30/50 [03:01<01:50,  5.52s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape https://ieeexplore.ieee.org/document/9645441, error: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"article\"]\"}\n",
      "  (Session info: headless chrome=107.0.5304.110)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010267b2c8 chromedriver + 4752072\n",
      "1   chromedriver                        0x00000001025fb463 chromedriver + 4228195\n",
      "2   chromedriver                        0x000000010225eb18 chromedriver + 441112\n",
      "3   chromedriver                        0x000000010229be21 chromedriver + 691745\n",
      "4   chromedriver                        0x000000010229c061 chromedriver + 692321\n",
      "5   chromedriver                        0x00000001022d75e4 chromedriver + 935396\n",
      "6   chromedriver                        0x00000001022bcd2d chromedriver + 826669\n",
      "7   chromedriver                        0x00000001022d5134 chromedriver + 926004\n",
      "8   chromedriver                        0x00000001022bcb33 chromedriver + 826163\n",
      "9   chromedriver                        0x000000010228d9fd chromedriver + 633341\n",
      "10  chromedriver                        0x000000010228f051 chromedriver + 639057\n",
      "11  chromedriver                        0x000000010264830e chromedriver + 4543246\n",
      "12  chromedriver                        0x000000010264ca88 chromedriver + 4561544\n",
      "13  chromedriver                        0x00000001026546df chromedriver + 4593375\n",
      "14  chromedriver                        0x000000010264d8fa chromedriver + 4565242\n",
      "15  chromedriver                        0x00000001026232cf chromedriver + 4391631\n",
      "16  chromedriver                        0x000000010266c5b8 chromedriver + 4691384\n",
      "17  chromedriver                        0x000000010266c739 chromedriver + 4691769\n",
      "18  chromedriver                        0x000000010268281e chromedriver + 4782110\n",
      "19  libsystem_pthread.dylib             0x00007ff816237259 _pthread_start + 125\n",
      "20  libsystem_pthread.dylib             0x00007ff816232c7b thread_start + 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping papers from nlp_papers.txt:  68%|██ | 34/50 [03:23<01:31,  5.74s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape https://ieeexplore.ieee.org/document/9795286, error: Could not load body of paper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Scraping papers from nlp_papers.txt:  70%|██ | 35/50 [03:26<01:16,  5.10s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape https://ieeexplore.ieee.org/document/9194384, error: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"article\"]\"}\n",
      "  (Session info: headless chrome=107.0.5304.110)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010267b2c8 chromedriver + 4752072\n",
      "1   chromedriver                        0x00000001025fb463 chromedriver + 4228195\n",
      "2   chromedriver                        0x000000010225eb18 chromedriver + 441112\n",
      "3   chromedriver                        0x000000010229be21 chromedriver + 691745\n",
      "4   chromedriver                        0x000000010229c061 chromedriver + 692321\n",
      "5   chromedriver                        0x00000001022d75e4 chromedriver + 935396\n",
      "6   chromedriver                        0x00000001022bcd2d chromedriver + 826669\n",
      "7   chromedriver                        0x00000001022d5134 chromedriver + 926004\n",
      "8   chromedriver                        0x00000001022bcb33 chromedriver + 826163\n",
      "9   chromedriver                        0x000000010228d9fd chromedriver + 633341\n",
      "10  chromedriver                        0x000000010228f051 chromedriver + 639057\n",
      "11  chromedriver                        0x000000010264830e chromedriver + 4543246\n",
      "12  chromedriver                        0x000000010264ca88 chromedriver + 4561544\n",
      "13  chromedriver                        0x00000001026546df chromedriver + 4593375\n",
      "14  chromedriver                        0x000000010264d8fa chromedriver + 4565242\n",
      "15  chromedriver                        0x00000001026232cf chromedriver + 4391631\n",
      "16  chromedriver                        0x000000010266c5b8 chromedriver + 4691384\n",
      "17  chromedriver                        0x000000010266c739 chromedriver + 4691769\n",
      "18  chromedriver                        0x000000010268281e chromedriver + 4782110\n",
      "19  libsystem_pthread.dylib             0x00007ff816237259 _pthread_start + 125\n",
      "20  libsystem_pthread.dylib             0x00007ff816232c7b thread_start + 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping papers from nlp_papers.txt: 100%|███| 50/50 [04:58<00:00,  5.97s/paper]\n"
     ]
    }
   ],
   "source": [
    "nlp_papers = scrape_papers(nlp_paper_urls)\n",
    "\n",
    "# save the papers to a json file\n",
    "with open('nlp_papers.json', 'w') as f:\n",
    "    json.dump(nlp_papers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Statistics\n",
    "\n",
    "We have approximately 50 NLP and CNN papers. Here are some statistics about the papers in our dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 98\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('cnn_papers.json', 'r') as f:\n",
    "    cnn_papers = json.load(f)\n",
    "\n",
    "with open('nlp_papers.json', 'r') as f:\n",
    "    nlp_papers = json.load(f)\n",
    "\n",
    "dataset = cnn_papers + nlp_papers\n",
    "\n",
    "abstracts = [paper['abstract'] for paper in dataset]\n",
    "introductions = [paper['section_texts'][0] for paper in dataset]\n",
    "paper_bodies = [' '.join(paper['section_texts']) for paper in dataset]\n",
    "\n",
    "print(f'Total number of papers: {len(dataset)}') # if this isn't 100, that's okay because some papers failed to scrape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in abstract: 307\n",
      "Min number of words in abstract: 72\n",
      "Average number of words in abstract: 209.39795918367346\n",
      "Median number of words in abstract: 206\n"
     ]
    }
   ],
   "source": [
    "print(f'Max number of words in abstract: {max([len(abstract.split()) for abstract in abstracts])}')\n",
    "print(f'Min number of words in abstract: {min([len(abstract.split()) for abstract in abstracts])}')\n",
    "print(f'Average number of words in abstract: {sum([len(abstract.split()) for abstract in abstracts]) / len(abstracts)}')\n",
    "print(f'Median number of words in abstract: {sorted([len(abstract.split()) for abstract in abstracts])[len(abstracts) // 2]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in introduction: 3913\n",
      "Min number of words in introduction: 435\n",
      "Average number of words in introduction: 1010.2857142857143\n",
      "Median number of words in introduction: 947\n",
      "Upper 90 percentile number of words in introduction: 1538\n"
     ]
    }
   ],
   "source": [
    "print(f'Max number of words in introduction: {max([len(introduction.split()) for introduction in introductions])}')\n",
    "print(f'Min number of words in introduction: {min([len(introduction.split()) for introduction in introductions])}')\n",
    "print(f'Average number of words in introduction: {sum([len(introduction.split()) for introduction in introductions]) / len(introductions)}')\n",
    "print(f'Median number of words in introduction: {sorted([len(introduction.split()) for introduction in introductions])[len(introductions) // 2]}')\n",
    "print(f'Upper 90 percentile number of words in introduction: {sorted([len(introduction.split()) for introduction in introductions])[int(len(introductions) * 0.9)]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in paper: 17011\n",
      "Min number of words in paper: 2796\n",
      "Average number of words in paper: 7296.071428571428\n",
      "Median number of words in paper: 6808\n",
      "Upper 90 percentile number of words in paper: 11075\n"
     ]
    }
   ],
   "source": [
    "print(f'Max number of words in paper: {max([len(paper_body.split()) for paper_body in paper_bodies])}')\n",
    "print(f'Min number of words in paper: {min([len(paper_body.split()) for paper_body in paper_bodies])}')\n",
    "print(f'Average number of words in paper: {sum([len(paper_body.split()) for paper_body in paper_bodies]) / len(paper_bodies)}')\n",
    "print(f'Median number of words in paper: {sorted([len(paper_body.split()) for paper_body in paper_bodies])[len(paper_bodies) // 2]}')\n",
    "print(f'Upper 90 percentile number of words in paper: {sorted([len(paper_body.split()) for paper_body in paper_bodies])[int(len(paper_bodies) * 0.9)]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}