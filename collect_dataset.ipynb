{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cnn_paper_urls = 'cnn_papers.txt'\n",
    "nlp_paper_urls = 'nlp_papers.txt'\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome('chromedriver',options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_body(body):\n",
    "    \"\"\"This function takes the body of the paper and returns three lists: the section numbers (in roman numerals), the section titles, and the section text\n",
    "    It was written entirely by GitHub Copilot\"\"\"\n",
    "    sections = body.split('SECTION ')\n",
    "\n",
    "    sections = sections[1:] # remove the first element, which is just an empty string due to how we split the text\n",
    "    section_nums = []\n",
    "    section_titles = []\n",
    "    section_texts = []\n",
    "    for section in sections:\n",
    "        section_num = section.split('\\n')[0]\n",
    "        section_title = section.split('\\n')[1]\n",
    "        section_text = section.split('\\n')[2:]\n",
    "\n",
    "        section_text = ' '.join(section_text)\n",
    "        section_nums.append(section_num)\n",
    "        section_titles.append(section_title)\n",
    "        section_texts.append(section_text)\n",
    "\n",
    "    return section_nums, section_titles, section_texts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scrape_paper(url):\n",
    "    driver.get(url)\n",
    "\n",
    "    title = driver.find_element_by_class_name('document-title').text\n",
    "\n",
    "    # the 2nd u-mb-1 div is normally the abstract, but sometimes it is empty, so we have to handle that special case\n",
    "    abstract = driver.find_elements_by_class_name('u-mb-1')[1]\n",
    "    if abstract.text == '':\n",
    "        abstract = driver.find_elements_by_class_name('u-mb-1')[2]\n",
    "    abstract = abstract.find_elements_by_tag_name('div')[0].text\n",
    "\n",
    "    try:\n",
    "        body = driver.find_element('id', 'article').text\n",
    "    except:\n",
    "        time.sleep(1) # wait a second and try again, it's probably just an issue loading the page\n",
    "        body = driver.find_element('id', 'article').text\n",
    "\n",
    "    _, section_titles, section_texts = parse_body(body)\n",
    "\n",
    "    return title, abstract, section_titles, section_texts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scrape_papers(url_file):\n",
    "    with open(url_file, 'r') as f:\n",
    "        urls = f.readlines()\n",
    "    urls = [url.strip() for url in urls]\n",
    "\n",
    "    papers = []\n",
    "    for url in tqdm(urls, desc=f'Scraping papers from {url_file}', total=len(urls), unit='paper'):\n",
    "        try:\n",
    "            title, abstract, section_titles, section_texts = scrape_paper(url)\n",
    "            papers.append({'title': title, 'abstract': abstract, 'section_titles': section_titles, 'section_texts': section_texts})\n",
    "        except Exception as e:\n",
    "            print(f'Failed to scrape {url}, error: {e}')\n",
    "            continue\n",
    "\n",
    "    return papers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnn_papers = scrape_papers(cnn_paper_urls)\n",
    "\n",
    "# save the papers to a json file\n",
    "with open('cnn_papers.json', 'w') as f:\n",
    "    json.dump(cnn_papers, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp_papers = scrape_papers(nlp_paper_urls)\n",
    "\n",
    "# save the papers to a json file\n",
    "with open('nlp_papers.json', 'w') as f:\n",
    "    json.dump(nlp_papers, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}